{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6db2bff7-259f-4c7f-8ea4-27a644c21ca5",
      "metadata": {
        "id": "6db2bff7-259f-4c7f-8ea4-27a644c21ca5"
      },
      "source": [
        "### Install all the neccessaary package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bda117e9-b02a-402c-9751-690abe8a5127",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bda117e9-b02a-402c-9751-690abe8a5127",
        "outputId": "efa54219-e06b-483c-d158-726889e9a111",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (2.2.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: opencv-python in ./.venv/lib/python3.13/site-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in ./.venv/lib/python3.13/site-packages (from opencv-python) (2.2.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: opencv-contrib-python in ./.venv/lib/python3.13/site-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in ./.venv/lib/python3.13/site-packages (from opencv-contrib-python) (2.2.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install numpy\n",
        "%pip install opencv-python\n",
        "%pip install opencv-contrib-python"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66709a41-ff89-4bf2-b0a3-31eff758cc78",
      "metadata": {
        "id": "66709a41-ff89-4bf2-b0a3-31eff758cc78"
      },
      "source": [
        "### Import packages and libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1759c144-3d84-4da2-9302-265e6351e1bd",
      "metadata": {
        "id": "1759c144-3d84-4da2-9302-265e6351e1bd"
      },
      "outputs": [],
      "source": [
        "# A computer vision library\n",
        "import cv2\n",
        "# A scientific computation library\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e83ec207-5146-48b2-836e-3af8c2ab2f7b",
      "metadata": {
        "id": "e83ec207-5146-48b2-836e-3af8c2ab2f7b"
      },
      "source": [
        "### Test a camera"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "57de1ff4-be8b-442d-bfb5-6432e53fdab4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "57de1ff4-be8b-442d-bfb5-6432e53fdab4",
        "outputId": "04cfb0ae-14f3-4961-cfb4-21b9baea5324"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:01:30.954 Python[9814:91813] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'shape'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m vid\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Add text at bottom right\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClick \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to exit webcam\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m300\u001b[39m, frame\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m20\u001b[39m), \n\u001b[1;32m      8\u001b[0m             cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.7\u001b[39m, (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     10\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVideo\u001b[39m\u001b[38;5;124m\"\u001b[39m, frame)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ],
      "source": [
        "\n",
        "vid = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    ret, frame = vid.read()\n",
        "    \n",
        "    # Add text at bottom right\n",
        "    cv2.putText(frame, \"Click 'q' to exit webcam\", (frame.shape[1] - 300, frame.shape[0] - 20), \n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "    \n",
        "    cv2.imshow(\"Video\", frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2TGoB6ef_Q_a",
      "metadata": {
        "id": "2TGoB6ef_Q_a"
      },
      "source": [
        "### Task One:\n",
        "\n",
        "Instruct students to read videos stored on the computer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bc3be6c-61eb-4bf4-8083-3085ab49f19d",
      "metadata": {
        "id": "8bc3be6c-61eb-4bf4-8083-3085ab49f19d"
      },
      "source": [
        "### Face Detection\n",
        "\n",
        "This is to be able to draw a bounding box around the face.\n",
        "\n",
        "We are also gathering data for the training of the recogition. The code will request for a name, put a name in the box provided then press Enter. A window frame will pop up, and when it sees a face it will draw a green bounding as it has detected a face. When the bounding box is not shaking and it is stable, press q and you will be prompted for a name again. The process will be repeated all over again. We set training for five(5) people"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a9378463-2c1e-4227-aaea-428ad01014b3",
      "metadata": {
        "id": "a9378463-2c1e-4227-aaea-428ad01014b3"
      },
      "outputs": [],
      "source": [
        "cap = cv2.VideoCapture(0)\n",
        "face_detector = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
        "face_data = []\n",
        "labels = []\n",
        "number_of_people = 2\n",
        "for i in range(number_of_people):\n",
        "    name = input(f\"Enter the name of person {i+1}: \")\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        faces = face_detector.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
        "        for (x, y, w, h) in faces:\n",
        "            face = gray[y:y + h, x:x + w]\n",
        "            face = cv2.resize(face, (512, 512), interpolation=cv2.INTER_AREA)\n",
        "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 4)\n",
        "            cv2.putText(frame, \"Click 'q' to save image\", (frame.shape[1] - 300, frame.shape[0] - 20), \n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "            cv2.imshow('Frame', frame)\n",
        "\n",
        "            if 0xFF == ord('q'):\n",
        "                break\n",
        "\n",
        "        cv2.imshow('Frame', frame)\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "    face_data.append(face)\n",
        "    labels.append(name)\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hAH8UkW0_Z1O",
      "metadata": {
        "id": "hAH8UkW0_Z1O"
      },
      "source": [
        "# Train on different people both through webcam and videos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c27b7eb8-7fdc-4e72-8704-248704e1d894",
      "metadata": {
        "id": "c27b7eb8-7fdc-4e72-8704-248704e1d894"
      },
      "source": [
        "### Train a face recognition model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "90c32c83-08c4-4af9-8873-3a10ce7c1a98",
      "metadata": {
        "id": "90c32c83-08c4-4af9-8873-3a10ce7c1a98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 'Sadeeq', 1: 'Aliyu'} [array([[209, 205, 202, ..., 206, 217, 194],\n",
            "       [208, 202, 200, ..., 206, 217, 197],\n",
            "       [208, 202, 199, ..., 207, 215, 199],\n",
            "       ...,\n",
            "       [ 72,  73,  73, ..., 185, 219, 198],\n",
            "       [ 73,  73,  73, ..., 186, 220, 196],\n",
            "       [ 73,  73,  73, ..., 185, 223, 192]], shape=(512, 512), dtype=uint8), array([[222, 189, 203, ..., 223, 169, 220],\n",
            "       [223, 189, 204, ..., 224, 167, 223],\n",
            "       [224, 187, 206, ..., 225, 165, 225],\n",
            "       ...,\n",
            "       [194, 194, 194, ..., 199, 169, 236],\n",
            "       [193, 193, 193, ..., 197, 167, 232],\n",
            "       [193, 193, 193, ..., 199, 173, 232]], shape=(512, 512), dtype=uint8)]\n"
          ]
        }
      ],
      "source": [
        "# Create a LBPH face recognizer\n",
        "labels2={i:j for i,j in enumerate(labels)}\n",
        "\n",
        "print(labels2, face_data)\n",
        "face_recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
        "\n",
        "# Train the face recognizer on the dataset\n",
        "face_recognizer.train(face_data, np.array(list(labels2.keys())))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f48dfe8-5fe9-46f6-b1b4-5295e2bfea02",
      "metadata": {
        "id": "5f48dfe8-5fe9-46f6-b1b4-5295e2bfea02"
      },
      "source": [
        "### Face Recognition:\n",
        "\n",
        "This part of the code is to be able to detect the person that was stored. The code will detect the face of anyone stored and it will be unknown if the person is not part of the data. Press q when done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "05460a9f-3fcf-4078-9dcd-3632c071ae9a",
      "metadata": {
        "id": "05460a9f-3fcf-4078-9dcd-3632c071ae9a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Real-time face recognition from webcam\n",
        "cap = cv2.VideoCapture(0)\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_detector.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "        face_roi = gray[y:y+h, x:x+w]\n",
        "        face_features = cv2.resize(face, (512, 512), interpolation=cv2.INTER_AREA)\n",
        "        label, confidence = face_recognizer.predict(face_features)\n",
        "        print(confidence)\n",
        "        detection_name = labels2[label] if confidence > 0.6 else \"Unknown\"\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "        cv2.putText(frame, str(labels2[label]), (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
        "\n",
        "    cv2.imshow('Face Recognition', frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RkPbmL83D_7_",
      "metadata": {
        "id": "RkPbmL83D_7_"
      },
      "outputs": [],
      "source": [
        "# Task on how to save models using cv.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fgpc-OjWEFNb",
      "metadata": {
        "id": "fgpc-OjWEFNb"
      },
      "outputs": [],
      "source": [
        "# Take home task, create a facial authentication system on streamlit\n",
        "\n",
        "# Register face(s): Sign in\n",
        "# Login using Face registered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jg0yoekCESX8",
      "metadata": {
        "id": "jg0yoekCESX8"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
